from utils.define_repartition import get_file_size_bytes, calculate_partitions
from utils.create_postgres_table import create_table_if_not_exists
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, month, dayofmonth, to_timestamp, current_timestamp
import os
import logging

class SourceToDb:
    def __init__(self):
        
        self.local_path  = os.getenv('DATA_SOURCE_PATH')
        self.raw_path = os.getenv('RAW_PATH')
        
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # Initialisation de la session Spark avec support Hive
        self.spark = SparkSession.builder \
            .appName("Feeder_Source_to_HDFS") \
            .enableHiveSupport() \
            .getOrCreate()
            
        self.spark.sparkContext.setLogLevel("WARN")
        self.logger.info("‚úÖ SparkSession initialis√©e avec succ√®s.")

    def read_source_data(self, full_table_path):
        """
        Lecture des donn√©es source depuis le syst√®me de fichiers sans sch√©ma explicite.
        Ajoute une colonne 'ingestion_date' avec le timestamp actuel.
        Retourne un DataFrame Spark.
        """
        try:
            self.logger.info(f"Lecture du CSV depuis : {full_table_path}")
            return (
                self.spark
                .read
                .option("header", True)
                .option("inferSchema", True)
                .csv(full_table_path)
            )
        
        except Exception as e:
            self.logger.error(f"Erreur lors de la lecture des donn√©es sources : {e}")
            raise RuntimeError(f"Erreur lors de la lecture des donn√©es sources : {e}")

    def write_to_postgresql(self, df, table_name):
        """
        √âcrit les donn√©es dans une table PostgreSQL via JDBC en mode append.
        Remplace la connexion avec les variables d'environnement ou ta config.
        """
        try:
            jdbc_url = os.getenv("POSTGRES_JDBC_URL", "jdbc:postgresql://postgres:5432/source_data")
            jdbc_user = os.getenv("POSTGRES_USER", "hive")
            jdbc_password = os.getenv("POSTGRES_PASSWORD", "hive")
            jdbc_driver = "org.postgresql.Driver"
            
            create_table_if_not_exists(df, table_name, jdbc_url, jdbc_user, jdbc_password, host="postgres", port=5432)

            self.logger.info(f"D√©but de l'√©criture de la table PostgreSQL '{table_name}'.")
            
            (
                df.write
                .format("jdbc")
                .mode("overwrite")
                .option("url", jdbc_url)
                .option("dbtable", table_name)
                .option("user", jdbc_user)
                .option("password", jdbc_password)
                .option("driver", jdbc_driver)
                .save()
            )

            self.logger.info(f"Donn√©es ajout√©es √† la table PostgreSQL '{table_name}'.")
        except Exception as e:
            self.logger.error(f"Erreur lors de l'√©criture vers PostgreSQL : {e}")
            raise RuntimeError(f"Erreur lors de l'√©criture vers PostgreSQL : {e}")


    def run_pipeline(self):
        """
        Ex√©cute la pipeline compl√®te pour transformer les fichiers dans un dossier local.

        Args:
            crypto_name (str): Nom de la cryptomonnaie (peut √™tre utilis√© comme nom de table).
        """
        try:
            datasource = self.local_path
            files = [f for f in os.listdir(datasource) if os.path.isfile(os.path.join(datasource, f))]

            if not files:
                self.logger.warning("Aucun fichier trouv√© dans le dossier source.")
                return

            for file_name in files:
                source_file_path = f"file://{os.path.join(datasource, file_name)}"
                # source_file_path = f"{datasource}{file_name}"
                self.logger.info(f"\n--- Traitement du fichier : {source_file_path} ---")

                raw_data = self.read_source_data(source_file_path)

                # Utilise le nom de fichier sans extension comme nom de table Hive
                table_name = os.path.splitext(file_name)[0]
                
                table_name = 'transactions_data' if table_name.startswith('transactions_data') else table_name
                
                self.write_to_postgresql(raw_data, table_name)

                self.logger.info(f"‚úî Pipeline ex√©cut√©e avec succ√®s pour '{file_name}'.")

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'ex√©cution de la pipeline : {e}")
            
        finally:
            self.logger.info("üõë Fermeture de la session Spark.")
            self.spark.stop()

if __name__ == "__main__":
    SourceToDb().run_pipeline()
