x-pipeline-defaults: &pipeline-template
  build:
    context: .
    dockerfile: Dockerfile
  volumes:
    - ./:/app/application
    - C:/Users/julie/Documents/M1/BIG DATA FRAMEWORK 2/datasource:/app/datasource
  ports:
    - "4040:4040"
  command: ["/app/application/entrypoint.sh"]
  networks:
    - hadoopnetwork
  deploy:
    resources:
      limits:
        memory: 4g
        cpus: '2.0'

x-common-env: &common-env
  POSTGRES_JDBC_URL: jdbc:postgresql://postgres:5432/datamart
  DATA_SOURCE_PATH: hdfs://namenode:9000/user/root/datasource/datasource/ # ou file:///app/datasource/
  RAW_PATH: hdfs://namenode:9000/RAW/
  WAREHOUSE_PATH: hdfs://namenode:9000/user/hive/warehouse/
  WAREHOUSE_DB: process_data
  WAREHOUSE_DB_PATH: hdfs://namenode:9000/user/hive/warehouse/process_data.db/
  JDBC_USER: hive
  JDBC_PASSWORD: hive

services:

  pipeline-all-step:
    <<: *pipeline-template
    container_name: pipeline_all_step
    environment:
      <<: *common-env
      PY_FILE: main.py

  pipeline-step-0:
    <<: *pipeline-template
    container_name: source_to_db
    environment:
      <<: *common-env
      DATA_SOURCE_PATH: hdfs://namenode:9000/user/root/datasource/datasource/to_db/global_file/ # ou file:///app/datasource/to_db/global_file/
      POSTGRES_JDBC_URL: jdbc:postgresql://postgres:5432/source_data
      PY_FILE: source_to_bdd.py

  pipeline-step-1:
    <<: *pipeline-template
    container_name: feeder_file
    environment:
      <<: *common-env
      PY_FILE: feeder_file.py

  pipeline-step-2:
    <<: *pipeline-template
    container_name: feeder_jdbc
    environment:
      <<: *common-env
      POSTGRES_JDBC_URL: jdbc:postgresql://postgres:5432/source_data
      PY_FILE: feeder_jdbc.py

  pipeline-step-3:
    <<: *pipeline-template
    container_name: preprocessing
    environment:
      <<: *common-env
      PY_FILE: preprocessing.py

  pipeline-step-4:
    <<: *pipeline-template
    container_name: datamart
    environment:
      <<: *common-env
      PY_FILE: datamart.py

  pipeline-step-5:
    <<: *pipeline-template
    container_name: machine_learning
    environment:
      <<: *common-env
      PY_FILE: ml.py

networks:
  hadoopnetwork:
    driver: bridge
    name: "hadoopnetwork"
    ipam:
      config:
        - subnet: "172.20.0.0/24"

